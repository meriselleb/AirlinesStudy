{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.17.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Collecting seaborn\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.17.4)\n",
      "Collecting matplotlib>=1.4.3 (from seaborn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/42/3e92d7aa64295483fbca20a86c89b34d0cb43cffaadaffe028793902d790/matplotlib-3.1.2-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 176kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.15.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (0.25.3)\n",
      "Collecting scipy>=0.14.0 (from seaborn)\n",
      "\u001b[33m  WARNING: Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/18/f788042dc7a73d0176e073909c00a6765c953986cae6d8cc13782a4bcd05/scipy-1.3.3-cp37-cp37m-manylinux1_x86_64.whl (25.2MB)\n",
      "\u001b[K     |████████████████████████████████| 25.2MB 3.5MB/s eta 0:00:011    |████▍                           | 3.4MB 11.5MB/s eta 0:00:02"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install seaborn\n",
    "# ^ comment out if pandas and seaborn  not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There is a common belief that if an airline has a past history of having many incidents, that they will have a higher chance of having more incidents in the future compared to airlines that do not have a past history of having many incidents. Personally, whenever I travel, while I am looking for the most affordable plane ticket, I also want to make sure that the airline I will be using is a safe one. One of the first things I do is search up the airline to see if any news articles of any recent accidents with the airline appear. I also search up whether the airline has a good track record of being safe and whether there have been many incidents involving the airline. However, is an airlines history of incidents really a good indication of whether the airline will be less likely or more likely to have future incidents?\n",
    "\n",
    "    For my project, I will be comparing and studying two data sets. One of which is a data set of the amount of incidents by airline from 1985-1999 and the other being a data set of the amount of incidents by airline from 2000-2014. I will be comparing the data sets to see if airlines that had 'high' amounts of incidents from 1985-1999 also had 'high' amounts of incidents from 2000-2014 to test this common belief. My null hypothesis for my study is the common belief that airlines that had 'high' amount of incidents from 1985-1999 will also have 'high' amount of incidents from 2000-2014 compared to those who did not have high amounts of incidents in 1985-1999. With this data, I will be able to see if a history of past incidents (the dataset from 1985-1999) has any correlation on the likelihood of future incidents (the dataset from 2000-2014).\n",
    "\n",
    "    I will also test my hypothesis by doing the percentage of incidents compared to the total amount of seats the airline sells each week to give a percent likelihood of the person purchasing the seat will get into an accident. My main study is simply comparing the 1985-1999 accidents and 2000-2014 accidents and drawing connections. However, I also want to see if the percent likelihood will also correlate with the relationships I find in analysis. I think it'll be an interesting way to see how certain data and data comparisons can result to the same or different conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # checking if the data was successfully read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTIVE ANALYSIS:\n",
    "   Before beginning my main anaylsis of the data, there is general descriptive anaylsis that can be done with the data as it is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTIVE ANALYSIS:\n",
    "# Calculating the total amount of incidents for each data set\n",
    "TotalEarlier = df['incidents_85_99'].sum()\n",
    "print(\"Totals:\")\n",
    "print (\"Total Incidents from 1985-1999: \", TotalEarlier)\n",
    "TotalRecent = df['incidents_00_14'].sum()\n",
    "print (\"Total Incidents from 2000-2014: \", TotalRecent)\n",
    "# Finding the overall average amount of incidents per airline in each of the respective data sets\n",
    "print(\"Means:\")\n",
    "earlierMean = np.mean(df['incidents_85_99'])\n",
    "print(\"From 1985-1999, the average amount of incidents per airline was: \", earlierMean)\n",
    "recentMean = np.mean(df['incidents_00_14'])\n",
    "print(\"From 2000-2014, the average amount of incidents per airline was: \", recentMean)\n",
    "# Calculating Medians\n",
    "print(\"Medians:\")\n",
    "earlierMed = np.median(df['incidents_85_99'])\n",
    "recentMed = np.median(df['incidents_00_14'])\n",
    "print(\"From 1985-1999, the median amount of incidents per airline was: \", earlierMed)\n",
    "print(\"From 2000-2014, the median amount of incidents per airline was: \", recentMed)\n",
    "# Calculating Variance\n",
    "earlierVar = np.var(df['incidents_85_99'])\n",
    "recentVar = np.var(df['incidents_00_14'])\n",
    "print(\"Variance:\")\n",
    "print(\"The variance of the data set from 1985-1999: \", earlierVar)\n",
    "print(\"The variance of the data set from 2000-2014: \", recentVar)\n",
    "# Calculating Standard Deviation\n",
    "earlierstd = np.std(df['incidents_85_99'])\n",
    "recentstd = np.std(df['incidents_00_14'])\n",
    "print(\"Standard Deviation\")\n",
    "print(\"The standard Deviation of the data set from 1985-1999: \", earlierstd)\n",
    "print(\"The standard Deviation of the data set from 2000-2014: \", recentstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the airline with the highest amount of incidents from 1985-1999\n",
    "df[df['incidents_85_99']==df['incidents_85_99'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the airline with the highest amount of incidents from 2000-2014\n",
    "df[df['incidents_00_14']==df['incidents_00_14'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the airlines with the least amount of incidents 1985-1999\n",
    "df[df['incidents_85_99']==df['incidents_85_99'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the airlines with the least amount of incidents 2000-2014\n",
    "df[df['incidents_00_14']==df['incidents_00_14'].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          Overall, the total amount of incidents has gone down during 2000-2014 (with 231 total incidents) compared to the amount of incidents during 1985-1999 (with 402 total incidents). From 1985-1999, the airline with the highest amount of incidents was Aeroflot with 76 incidents while the airline with the highest amount of incidents from 2000-2014 was Delta/Northwest with 24 incidents. From 1985-1999, there were only three airlines without any incidents, those being Cathay Pacific, Hawaiian Airlines, and TAP-Air Portugal. While from 2000-2014, there were 9 airlines with without any incidents during the time period. \n",
    "        \n",
    "    As you can see when comparing the earlier dataset (1985-1999) and the more recent dataset (2000-2014), there was a decrease of total incidents, as well as an decrease in the highest amount of incidents for a single airline, and an increase in the amount of airlines with no incidents. \n",
    "    \n",
    "    However, while this data would naturally make sense (due to the progression of technology, one would assume newer tech would make newer planes safer than the ones being used in the 1985-1999 period), we still cannot say these statistics properly represent the data sets and actually support that assumption. One reason I am hesitant to do so is because of the suspected outlier in the 1985-1999 dataset of Aeroflot that had an unusally high amount of incidents (76), much higher than the average of 7.18 for that data set. The median (which is resistant to outliers unlike the mean, range, variance, and standard deviation) for the 1985-1999 was 4 while the median for the 2000-2014 data was 3. This is a incredibly small difference. Thus, we cannot be sure if this potential outlier is the reason for why the earlier data set has much higher incident statistics than the more recent data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating plot\n",
    "plt.figure(figsize=(100,40))\n",
    "plt.plot(df['airline'], df['incidents_85_99'], df['incidents_00_14'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     In this first data visualization, which you must zoom in to see fully, I compare the two data sets based on the amount of incidents each airline had from 1985-1999 (in blue) and from 2000-2014 (in orange). The purpose here is to show if the airlines that have had a history in a large amount of incidents continued to have a higher amount of incidents than those who did not have a history of a large amount of incidents. As you can see, there seems to be an overall similarity between the earlier set of data (1985-1999) and the most recent set of data (2000-2014) with the majority of the airlines. However, this simple plot only touches the surface and is not enough to make any conclusions. \n",
    "     \n",
    "     I will now create box plots for each of the data sets to see if there are any outliers present that may have had an influence on the statistics previously calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a box plot for 1985-1999\n",
    "plt.boxplot(x=df['incidents_85_99'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for 2000-2014\n",
    "plt.boxplot(x=df['incidents_00_14'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Looking at the two box plots, we are able to see what outliers each set of data has. As you can see, as suspected, the 76 incidents of Aeroflot is an outlier, and an incredibly far out outlier at that. While both sets do have their own outliers, the earlier data set has more outliers and outliers that are much farther away than the more recent data set.\n",
    "    Next, I will see if the data from 1985-1999 and from 2000-2014 were different with a Two-Sided Test. My null hypothesis in thi case is that there is a signficant difference between the earlier data set and the more recent data set in terms of total number of incidents. The alternative hypothesis is that there is no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlierIncidents = df['incidents_85_99']\n",
    "recentIncidents = df['incidents_00_14']\n",
    "meanDiff = np.mean(recentIncidents) - np.mean(earlierIncidents)\n",
    "print(\"Difference in Means: \", meanDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pooled_variance(a, b):\n",
    "    return ((((len(a) - 1) * np.var(a, ddof=1)) + ((len(b) - 1) * np.var(b, ddof=1))) / (len(a) + len(b) - 2)) * ((1/len(a)) + 1/len(b))\n",
    "pooled_var = custom_pooled_variance(earlierIncidents, recentIncidents)\n",
    "print('Pooled Variance:', np.round(pooled_var, 2))\n",
    "print('Pooled Standard Deviation:', np.round(np.sqrt(pooled_var), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthOf = len(earlierIncidents)\n",
    "degreesFree = len(earlierIncidents) + len(recentIncidents) -2\n",
    "print(\"Degrees of Freedom: \", degreesFree)\n",
    "dist = stats.t(loc=0,scale=np.sqrt(pooled_var), df = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Sided Test at a signifance of 0.05. Rejecting anything before 2.5th percentile and after 97.5th percentile\n",
    "# (the Two-Tail Critical Regions)\n",
    "lowerPercentile = dist.ppf(0.025)\n",
    "upperPercentile = dist.ppf(0.975)\n",
    "print(\"Rejection Region Lower Bounds: Anything Less than: \", np.round(lowerPercentile, 2))\n",
    "print(\"Rejection Region Upper Bounds: Anything above: \", np.round(upperPercentile,2))\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "x = np.linspace(dist.ppf(0.0001), dist.ppf(0.9999), 100)\n",
    "plt.plot(x, dist.pdf(x))\n",
    "plt.plot([dist.ppf(0.025), dist.ppf(0.025)],[0, dist.pdf(dist.ppf(0.025))], linestyle='dotted', color='#1f77b4')\n",
    "X_fill = np.linspace(dist.ppf(0.0001), dist.ppf(0.025))\n",
    "Y = dist.pdf(X_fill)\n",
    "plt.fill_between(X_fill, 0, Y, facecolor='red')\n",
    "plt.plot([dist.ppf(0.975), dist.ppf(0.975)],[0, dist.pdf(dist.ppf(0.975))], linestyle='dotted', color='#1f77b4')\n",
    "X_fill = np.linspace(dist.ppf(0.975), dist.ppf(0.9999))\n",
    "Y = dist.pdf(X_fill)\n",
    "plt.fill_between(X_fill, 0, Y, facecolor='red')\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('Difference in Sample Means')\n",
    "plt.ylabel('Probability Density Function')\n",
    "fig.suptitle('T-Distribution of Difference Between Samples', fontsize=15, y=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P-Value\n",
    "stats.t(loc=0, scale=np.sqrt(pooled_var), df=degreesFree).cdf(meanDiff) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Since the p-value is 0.05814, the difference in the data is not statistically significant and indicates that the difference in the earlier data set and the more recent data set. We can not reject the null hypothesis that there is a signifcant difference, and we can not accept the alternative hypothesis that there is no significant difference. So, despite the mean data showing that there was a difference between the two means, after doing the Two Sided Test, there seems to be weak evidence against and for both the null and alternative hypothesis. In this case, due to the outliers, the median was a better representation of the data as the medians of both data sets were relatively similar and did not give significant evidence on whether the data sets were the same or different. \n",
    "    However, while this may not support or reject our hypothesis that there is a significant difference between the two data sets, this might help support our main hypothesis stated earlier in that the airlines with a history of incidents in the past will have a higher likelihood of incidents in the future. This is because, if the data sets can not be concluded to be different, they may have the same or similar distribution. If they do, that means there is a correlation between the data sets. That means that we can reasonably expect similar distributions in future data sets. Which, in turn, would be us accepting our main null hypothesis (that airlines with a history of a higher amount of incidents from the earlier data set have a higher amount of incidents in the more recent data set than airlines that did not have a higher amount of incidents in the earlier data set). \n",
    "    We will now do a Chi-Square Analysis for Homogenity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rejection Region: Greater than', np.round(dist.ppf(0.95), degreesFree))\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "x = np.linspace(dist.ppf(0.0001), dist.ppf(0.9999), 100)\n",
    "plt.plot(x, dist.pdf(x))\n",
    "plt.plot([dist.ppf(0.95), dist.ppf(0.95)],[0, dist.pdf(dist.ppf(0.95))], linestyle='dotted', color='#1f77b4')\n",
    "X_fill = np.linspace(dist.ppf(0.95), dist.ppf(0.999))\n",
    "Y = dist.pdf(X_fill)\n",
    "plt.fill_between(X_fill, 0, Y, facecolor='red')\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('Chi-Square Statistic')\n",
    "plt.ylabel('Probability Density Function')\n",
    "fig.suptitle('Chi-Square Distribution with 10 Degrees of Freedom', fontsize=15, y=0.92)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
